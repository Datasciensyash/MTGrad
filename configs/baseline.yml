experiment_name: Baseline-MTGrad
experiments_path: ./experiments/

optimizer_name: Adam
optimizer_params:
  weight_decay: 0.005

scheduler_name: StepLR
scheduler_params:
  step_size: 500
  gamma: 0.96

model_params:
  hidden_channels: 256
  num_decoder_blocks: 6
  num_attention_heads: 4

train_dataset_params:
  size: [16, 12]
  num_layers_range: [2, 6]
  powers_range: [60, 90]
  resistivity_range: [1, 20000]
  alpha_range: [0.1, 0.045]
  period_range: [0.001, 100]
  period_count_range: [12, 15]
  batch_size: 16
  epoch_size: 1000

valid_dataset_params:
  size: [16, 12]
  num_layers_range: [2, 6]
  powers_range: [60, 90]
  resistivity_range: [1, 20000]
  alpha_range: [0.1, 0.045]
  period_range: [0.001, 100]
  period_count_range: [12, 15]
  batch_size: 16
  epoch_size: 1

dataloader_params:
  train_batch_size: 16
  val_batch_size: 16
  num_workers: 0

model_checkpoint_params:
  monitor: train/loss
  filename: checkpoint-epoch{epoch:02d}-valid_loss{valid/loss:.2f}
  auto_insert_metric_name: False
  save_last: True
  save_top_k: 3

trainer_params:
  gpus: 1
  max_epochs: 2000
  check_val_every_n_epoch: 1
  accumulate_grad_batches: 1