experiment_name: Version1
experiments_path: ./experiments/

optimizer_name: Adam
optimizer_params: {}

scheduler_name: StepLR
scheduler_params:
  step_size: 100
  gamma: 0.95

model_params:
  hidden_channels: 64
  num_decoder_blocks: 4
  num_attention_heads: 4

train_dataset_params:
  layer_power_max: [150, 150, 150, 150]
  layer_power_min: [15, 15, 15, 15]
  layer_resistivity_min: [ 0, 0, 0, 0 ]
  layer_resistivity_max: [ 20000, 20000, 20000, 20000 ]
  size: 12
  pixel_size: 60
  period_range: [0.001, 100]
  period_count_range: [4, 6]
  batch_size: 16
  epoch_size: 100


valid_dataset_params:
  layer_power_max: [150, 150, 150, 150]
  layer_power_min: [15, 15, 15, 15]
  layer_resistivity_min: [ 0, 0, 0, 0 ]
  layer_resistivity_max: [ 20000, 20000, 20000, 20000 ]
  size: 12
  pixel_size: 60
  period_range: [0.001, 100]
  period_count_range: [10, 12]
  batch_size: 16
  epoch_size: 10

dataloader_params:
  train_batch_size: 16
  val_batch_size: 16
  num_workers: 0

model_checkpoint_params:
  monitor: valid/loss
  filename: checkpoint-epoch{epoch:02d}-valid_loss{valid/loss:.2f}
  auto_insert_metric_name: False
  save_last: True
  save_top_k: 3

trainer_params:
  gpus: 1
  max_epochs: 2000
  check_val_every_n_epoch: 1
  accumulate_grad_batches: 1